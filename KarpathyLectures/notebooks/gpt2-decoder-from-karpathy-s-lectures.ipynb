{"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.11"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14343225,"sourceType":"datasetVersion","datasetId":9158157}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n----\n- this is a practise exercise to learn the workings of modern transformers used in LLMs\n- I followed the Karpathy's lectures on YouTube and made some changes as needed (for better efficiency)","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\ntorch.manual_seed(42)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2026-01-02T12:49:50.453793Z","iopub.execute_input":"2026-01-02T12:49:50.454179Z","iopub.status.idle":"2026-01-02T12:49:50.464099Z","shell.execute_reply.started":"2026-01-02T12:49:50.454148Z","shell.execute_reply":"2026-01-02T12:49:50.463109Z"},"trusted":true},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x77ff6d7c4450>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from dataclasses import dataclass\n\n@dataclass\nclass config:\n    block_size = 8 ##context window\n    batch_size = 4 ","metadata":{"execution":{"iopub.status.busy":"2026-01-02T12:49:50.773871Z","iopub.execute_input":"2026-01-02T12:49:50.774304Z","iopub.status.idle":"2026-01-02T12:49:50.780887Z","shell.execute_reply.started":"2026-01-02T12:49:50.774265Z","shell.execute_reply":"2026-01-02T12:49:50.779602Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"# Reading the input text","metadata":{}},{"cell_type":"code","source":"file_path = r'/kaggle/input/tiny-shakespeare-karpathys-repo/tiny_shakespeare.txt'\nwith open(file_path,'r') as f:\n    text = f.read()\nprint(text[:100])\nprint('='*10)\nprint(f'Length of the dataset (all the characters):{len(text)}')","metadata":{"execution":{"iopub.status.busy":"2026-01-02T12:49:52.239113Z","iopub.execute_input":"2026-01-02T12:49:52.239451Z","iopub.status.idle":"2026-01-02T12:49:52.248119Z","shell.execute_reply.started":"2026-01-02T12:49:52.239423Z","shell.execute_reply":"2026-01-02T12:49:52.247198Z"},"trusted":true},"outputs":[{"name":"stdout","text":"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou\n==========\nLength of the dataset (all the characters):1115394\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## Creating a vocabulary from the text\n- here karpathy used the set() constructor, which is the most efficient approach ","metadata":{}},{"cell_type":"code","source":"characters = sorted(set(text)) ##takes out all the unique characters and sorts it, returning a list\nvocabulary_size = len(characters)\n\nprint(f'Size of vocabulary: {vocabulary_size}')\nvocab = ''.join(characters)\nprint('Vocabulary:',vocab)","metadata":{"execution":{"iopub.status.busy":"2026-01-02T12:49:54.255637Z","iopub.execute_input":"2026-01-02T12:49:54.255967Z","iopub.status.idle":"2026-01-02T12:49:54.274788Z","shell.execute_reply.started":"2026-01-02T12:49:54.255940Z","shell.execute_reply":"2026-01-02T12:49:54.273750Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Size of vocabulary: 65\nVocabulary: \n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Tokenizing the text\n- we are building a character level model, for which we'll first start with a lookup table for charcters -> index and inverse lookkup table for index -> characters\n- ","metadata":{}},{"cell_type":"code","source":"ch2idx = {ch:idx for idx,ch in enumerate(characters)}\nidx2ch = {idx:ch for idx,ch in enumerate(characters)}\n\ndef encode(text:str):\n    return [ch2idx[ch] for ch in text]\n\ndef decode(ids:list):\n    return ''.join([idx2ch[idx] for idx in ids])\n\nexample = 'My name is Harikesh'\nprint(\n    f'Input: {example}',\n    '\\n',\n    f'Len of Input: {len(example)}'\n    '\\n'\n    f'Output(encoded): {encode(example)}'\n    '\\n'\n    f'Length of Encoded output: {len(encode(example))}'\n)","metadata":{"execution":{"iopub.status.busy":"2026-01-02T12:49:56.775364Z","iopub.execute_input":"2026-01-02T12:49:56.775729Z","iopub.status.idle":"2026-01-02T12:49:56.782868Z","shell.execute_reply.started":"2026-01-02T12:49:56.775700Z","shell.execute_reply":"2026-01-02T12:49:56.782024Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Input: My name is Harikesh \n Len of Input: 19\nOutput(encoded): [25, 63, 1, 52, 39, 51, 43, 1, 47, 57, 1, 20, 39, 56, 47, 49, 43, 57, 46]\nLength of Encoded output: 19\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"##creating the encoded representation of all the data in a tensor\ndata = torch.tensor(encode(text),dtype=torch.long)\nprint(data.shape)","metadata":{"execution":{"iopub.status.busy":"2026-01-02T12:49:58.777274Z","iopub.execute_input":"2026-01-02T12:49:58.778119Z","iopub.status.idle":"2026-01-02T12:49:58.935171Z","shell.execute_reply.started":"2026-01-02T12:49:58.778086Z","shell.execute_reply":"2026-01-02T12:49:58.934034Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([1115394])\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Splitting the data in train and val\n- here we can't use a random split, because the text follows a semantic order which has to be preserved if we want our model to learn to generate text like Shakespeare\n- we'll use the first 90% data for training and rest 10% for validation","metadata":{}},{"cell_type":"code","source":"n = int(0.9*len(data))\n\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"execution":{"iopub.status.busy":"2026-01-02T12:49:59.301381Z","iopub.execute_input":"2026-01-02T12:49:59.301795Z","iopub.status.idle":"2026-01-02T12:49:59.307614Z","shell.execute_reply.started":"2026-01-02T12:49:59.301757Z","shell.execute_reply":"2026-01-02T12:49:59.306567Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### Training Philosophy\n- any chunk of data with size 'd' that is sampled from the train data has 'd' number of training examples for the model\n- starting from the first character, we train the model to predict second then with the first two, the third and so on till the 'd-1'th element which is used to predict the d'th character\n- here the choice to take context size as 'd' is freely available to the user, with higher values of d requiring more compute","metadata":{}},{"cell_type":"code","source":"##example\nX = train_data[:config.block_size+1]\ny = train_data[1:config.block_size+1]\n\nfor t in range(config.block_size):\n    context = X[:1+t]\n    label = y[t]\n    print(f'for context: {context} -> {label}')","metadata":{"execution":{"iopub.status.busy":"2026-01-02T12:50:00.782209Z","iopub.execute_input":"2026-01-02T12:50:00.782580Z","iopub.status.idle":"2026-01-02T12:50:00.792428Z","shell.execute_reply.started":"2026-01-02T12:50:00.782547Z","shell.execute_reply":"2026-01-02T12:50:00.791259Z"},"trusted":true},"outputs":[{"name":"stdout","text":"for context: tensor([18]) -> 47\nfor context: tensor([18, 47]) -> 56\nfor context: tensor([18, 47, 56]) -> 57\nfor context: tensor([18, 47, 56, 57]) -> 58\nfor context: tensor([18, 47, 56, 57, 58]) -> 1\nfor context: tensor([18, 47, 56, 57, 58,  1]) -> 15\nfor context: tensor([18, 47, 56, 57, 58,  1, 15]) -> 47\nfor context: tensor([18, 47, 56, 57, 58,  1, 15, 47]) -> 58\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"### A note on generating the batch\n- karpathy's method randomly samples (batch_size,block_size) sample vectors from the input text\n- this stocasticity means that we may or may not use the whole dataset in training, while it maintains the IID assumptions and will be very beneficials for preventing overfitting there are other better ways to do it (in production)","metadata":{}},{"cell_type":"code","source":"def get_batch(data,context_window=config.block_size,batch_size=config.batch_size):\n    idx = torch.randint(len(data)-context_window,(batch_size,)) \n    X = torch.stack([data[i:i+context_window] for i in idx])\n    y = torch.stack([data[i+1:i+context_window+1] for i in idx])\n    return X,y\n\nxb,yb = get_batch(train_data)","metadata":{"execution":{"iopub.status.busy":"2026-01-02T12:50:02.775739Z","iopub.execute_input":"2026-01-02T12:50:02.776084Z","iopub.status.idle":"2026-01-02T12:50:02.803432Z","shell.execute_reply.started":"2026-01-02T12:50:02.776056Z","shell.execute_reply":"2026-01-02T12:50:02.802452Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"# BiGram Language Model\n- this model effictively uses the statistical approach of calculating all the bigrams from the input text and arranging them in a table where rows and columns are the first and second character of the bigrams and the intersecting cell is the count of how many times the column character follows the row charater in the text\n- in PyTorch we can implement this using the nn.Embedding() layer which createst a table of (vocab_size,vocab_size) assigning random numbers in the cells and upon training we hope that the values in these cells approach the count (normalized over the rows) in the statistical table approach","metadata":{}},{"cell_type":"code","source":"# %%writefile bigram.py\nclass Bigram(nn.Module):\n    def __init__(self,vocab_size):\n        super().__init__()\n        self.lookup_table = nn.Embedding(vocab_size,vocab_size)\n    def forward(self,idx,labels=None):\n        logits = self.lookup_table(idx) #(B,T,C) -> (4,8,65)\n\n        if labels is None:\n            loss = None\n        else:\n            B,T,C = logits.shape\n            logits = logits.view(B*T,C)\n            labels = labels.view(B*T)\n            loss = F.cross_entropy(logits,labels)\n        return logits,loss\n        \n    def generate(self,idx,max_length):\n        ##idx is (B,T) for batch and timesteps of the input context\n        for _ in range(max_length-1):\n            logits,loss = self(idx[:,-1])\n            # logits = logits[:,:]\n            probs = F.softmax(logits,dim=-1)\n            idx_next = torch.multinomial(probs,num_samples=1)\n            idx = torch.cat([idx,idx_next],dim=1)\n        return idx","metadata":{"execution":{"iopub.status.busy":"2026-01-02T12:50:03.702667Z","iopub.execute_input":"2026-01-02T12:50:03.702997Z","iopub.status.idle":"2026-01-02T12:50:03.710910Z","shell.execute_reply.started":"2026-01-02T12:50:03.702970Z","shell.execute_reply":"2026-01-02T12:50:03.709784Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Testing Untrained Bigram Model","metadata":{}},{"cell_type":"code","source":"##testing untrained bigram model\nbigram_model = Bigram(vocab_size=vocabulary_size)\nlogits,loss = bigram_model(xb,yb)\n\ninput_context = torch.zeros((1,1),dtype=torch.long)\noutputs = bigram_model.generate(input_context,100).squeeze(0).tolist()\nprint(len(outputs))\nprint(decode(outputs))","metadata":{"execution":{"iopub.status.busy":"2026-01-02T12:50:05.613090Z","iopub.execute_input":"2026-01-02T12:50:05.613484Z","iopub.status.idle":"2026-01-02T12:50:05.712270Z","shell.execute_reply.started":"2026-01-02T12:50:05.613451Z","shell.execute_reply":"2026-01-02T12:50:05.710935Z"},"trusted":true},"outputs":[{"name":"stdout","text":"100\n\ncfYCDRUZsYBsA?Y?vgB!ZWOEiAoezL:q&Avufr?gSGdWrp&Bxt-R?wo'TYhBChdIC-RDaRmEGENyouVg'UjyQNyQSpZUVeN:BZq\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"## Training the BiGram Model","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(bigram_model.parameters(),lr=1e-3)\n\nbatch_size=8\n\nfor steps in range(10000):\n    optimizer.step()\n    xb,yb = get_batch(train_data)\n\n    logits,loss = bigram_model(xb,yb)\n\n    loss.backward()\n    optimizer.step()\n\n    if steps%1000 == 0:\n        print(loss.item())\n\ninput_context = torch.zeros((1,1),dtype=torch.long)\noutputs = bigram_model.generate(input_context,100).squeeze(0).tolist()\n# print(len(outputs))\nprint(decode(outputs))","metadata":{"execution":{"iopub.execute_input":"2025-12-31T10:22:44.079633Z","iopub.status.busy":"2025-12-31T10:22:44.079053Z","iopub.status.idle":"2025-12-31T10:22:53.075447Z","shell.execute_reply":"2025-12-31T10:22:53.074254Z","shell.execute_reply.started":"2025-12-31T10:22:44.079599Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2.708622932434082\n","2.6894116401672363\n","2.176974058151245\n","2.289198160171509\n","4.675192832946777\n","2.432370662689209\n","2.631892442703247\n","2.386826753616333\n","2.491130828857422\n","2.5769729614257812\n","\n","G BENTCLOMNCI linn, onofeviapearb prstrde cooned insue'me wak hrs dotor LAno merareathabed hepe ar \n"]}],"execution_count":61},{"cell_type":"markdown","source":"# Understanding Attention Mechanism -  the **matrix multiplication trick**\n- before writing any self-attention block we need to understand what attention does\n- here we used a toy example to understand what attention does in practice","metadata":{}},{"cell_type":"code","source":"## a toy vector which represents the actual tensors in a language model \nB,T,C = 4,8,2\nx = torch.randn(B,T,C)\nx.shape","metadata":{"execution":{"iopub.status.busy":"2026-01-02T12:50:11.430388Z","iopub.execute_input":"2026-01-02T12:50:11.431588Z","iopub.status.idle":"2026-01-02T12:50:11.438204Z","shell.execute_reply.started":"2026-01-02T12:50:11.431534Z","shell.execute_reply":"2026-01-02T12:50:11.437390Z"},"trusted":true},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 2])"},"metadata":{}}],"execution_count":21},{"cell_type":"markdown","source":"- currently the T tokens in each batch are not communicating with each other, in the BiGram model only the current token was taken to look forward in the lookup table based on the probability\n- but we need them to interact in a certain way, to establish this interaction we begin with the simplest form of interaction\n- **Averaging all the tokens in channels till 't' where t <= T for each batch**","metadata":{}},{"cell_type":"markdown","source":"## 1. Using the naive loop","metadata":{}},{"cell_type":"code","source":"##code to do this operation also called BOW(bag of words)\nxbow = torch.zeros((B,T,C))\nfor batch_idx in range(B):\n    for time_step in range(T):\n        xprev = x[batch_idx,:time_step+1]\n        xbow[batch_idx,time_step] = torch.mean(xprev,0)\n\nx[0],xbow[0]","metadata":{"execution":{"iopub.status.busy":"2026-01-02T12:50:13.636286Z","iopub.execute_input":"2026-01-02T12:50:13.636639Z","iopub.status.idle":"2026-01-02T12:50:13.659241Z","shell.execute_reply.started":"2026-01-02T12:50:13.636611Z","shell.execute_reply":"2026-01-02T12:50:13.658158Z"},"trusted":true},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"(tensor([[ 1.0652,  1.4098],\n         [ 1.6263,  0.0065],\n         [ 0.0182,  0.1661],\n         [ 0.8874,  0.1085],\n         [ 1.3757, -0.2378],\n         [ 0.6894, -0.2131],\n         [-0.2155,  1.4510],\n         [-0.3396, -1.6361]]),\n tensor([[1.0652, 1.4098],\n         [1.3458, 0.7081],\n         [0.9032, 0.5274],\n         [0.8993, 0.4227],\n         [0.9946, 0.2906],\n         [0.9437, 0.2066],\n         [0.7781, 0.3844],\n         [0.6384, 0.1318]]))"},"metadata":{}}],"execution_count":22},{"cell_type":"markdown","source":"## 2. Using the matrix multiplication trick with **tril** or **triu**","metadata":{}},{"cell_type":"code","source":"mask = torch.tril(torch.ones(3,3))\nmask = mask/torch.sum(mask,1,keepdim=True)\nb = torch.randint(0,10,(3,2)).float()\nc = mask@b\n\nprint(f'mask=\\n{mask}')\nprint('='*5)\nprint(f'a:\\n{b}')\nprint('='*5)\nprint(f'mask @ a:\\n{c}')","metadata":{"execution":{"iopub.status.busy":"2026-01-02T12:50:15.990886Z","iopub.execute_input":"2026-01-02T12:50:15.991388Z","iopub.status.idle":"2026-01-02T12:50:16.014772Z","shell.execute_reply.started":"2026-01-02T12:50:15.991355Z","shell.execute_reply":"2026-01-02T12:50:16.013863Z"},"trusted":true},"outputs":[{"name":"stdout","text":"mask=\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n=====\na:\ntensor([[8., 8.],\n        [5., 4.],\n        [9., 1.]])\n=====\nmask @ a:\ntensor([[8.0000, 8.0000],\n        [6.5000, 6.0000],\n        [7.3333, 4.3333]])\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### Using this trick to create a similar matrix \n- here the mask (T,T) when matirx multiplied to x (B,T,C) by the rules of brodcasting mask is brodcasted to (B,T,T)\n- and like the previous cell all the matrices in channels are averaged ","metadata":{}},{"cell_type":"code","source":"mask = torch.tril(torch.ones(T,T))\nmask = mask/torch.sum(mask,1,keepdim=True)\n\nc = mask@x\nxbow[0],c[0]","metadata":{"execution":{"iopub.status.busy":"2026-01-02T12:50:18.128310Z","iopub.execute_input":"2026-01-02T12:50:18.128697Z","iopub.status.idle":"2026-01-02T12:50:18.140960Z","shell.execute_reply.started":"2026-01-02T12:50:18.128667Z","shell.execute_reply":"2026-01-02T12:50:18.140092Z"},"trusted":true},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"(tensor([[1.0652, 1.4098],\n         [1.3458, 0.7081],\n         [0.9032, 0.5274],\n         [0.8993, 0.4227],\n         [0.9946, 0.2906],\n         [0.9437, 0.2066],\n         [0.7781, 0.3844],\n         [0.6384, 0.1318]]),\n tensor([[1.0652, 1.4098],\n         [1.3458, 0.7081],\n         [0.9032, 0.5274],\n         [0.8993, 0.4227],\n         [0.9946, 0.2906],\n         [0.9437, 0.2066],\n         [0.7781, 0.3844],\n         [0.6384, 0.1318]]))"},"metadata":{}}],"execution_count":24},{"cell_type":"markdown","source":"## 3. Using the softmax and mask filling approach to make learnable parameters\n- here we will make the 'wei' matrix change based on the data\n- implementing that change in next step","metadata":{}},{"cell_type":"code","source":"tril = torch.tril(torch.ones(T,T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril==0,float('-inf'))\n\nwei = F.softmax(wei,dim=1)\n\nxbow3 = wei@x\n\ntorch.allclose(xbow,xbow3)","metadata":{"execution":{"iopub.status.busy":"2026-01-02T12:50:21.446462Z","iopub.execute_input":"2026-01-02T12:50:21.447570Z","iopub.status.idle":"2026-01-02T12:50:21.461718Z","shell.execute_reply.started":"2026-01-02T12:50:21.447520Z","shell.execute_reply":"2026-01-02T12:50:21.460897Z"},"trusted":true},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"## 4. Defining how to create the wei matrix from data dependent parameters\n- the output is aggregated per head (16 here)\n- usually this is done for multiple head sizes which add upto C","metadata":{}},{"cell_type":"code","source":"head_size = 16\nkey = nn.Linear(C,head_size,bias=False)\nquery = nn.Linear(C,head_size,bias=False)\nvalue = nn.Linear(C,head_size,bias=False)\n\n##defining wei matrix \n\nk = key(x) ## (B,T,C)*(C,h) -> (B,T,C)*(B,C,h) => (B,T,h)\nq = query(x) ## (B,T,h)\nv = value(x) ## (B,T,h)\n\nwei = q@k.transpose(-2,-1) ## (B,T,h)*(B,T,h) -> (B,T,h)*(B,h,T) => (B,T,T)\nwei = wei.masked_fill(tril==0,float('-inf'))\nn_wei = wei\nwei = F.softmax(wei,dim=2)\n\n# xbow_self_attention = wei@x\nxbow_self_attention = wei@v ## (B,T,h)\n##instead of directly applying softmax to x we apply it to value(x)\n\nprint(xbow_self_attention.shape)\n# print(torch.allclose(xbow_self_attention,xbow))","metadata":{"execution":{"iopub.status.busy":"2026-01-02T13:20:28.280064Z","iopub.execute_input":"2026-01-02T13:20:28.280456Z","iopub.status.idle":"2026-01-02T13:20:28.293885Z","shell.execute_reply.started":"2026-01-02T13:20:28.280421Z","shell.execute_reply":"2026-01-02T13:20:28.291995Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch.Size([4, 8, 16])\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"n_wei[0],wei[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T13:20:41.584151Z","iopub.execute_input":"2026-01-02T13:20:41.584759Z","iopub.status.idle":"2026-01-02T13:20:41.598790Z","shell.execute_reply.started":"2026-01-02T13:20:41.584717Z","shell.execute_reply":"2026-01-02T13:20:41.597700Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"(tensor([[-1.1803,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n         [-1.5551, -2.1457,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n         [-0.0365, -0.1489,  0.0055,    -inf,    -inf,    -inf,    -inf,    -inf],\n         [-0.8607, -1.2499, -0.0190, -0.6852,    -inf,    -inf,    -inf,    -inf],\n         [-1.2875, -1.6320, -0.0441, -0.9069, -1.3427,    -inf,    -inf,    -inf],\n         [-0.6344, -0.7472, -0.0255, -0.4186, -0.6070, -0.2945,    -inf,    -inf],\n         [ 0.0390, -0.8083,  0.0581, -0.3986, -0.7821, -0.4299,  0.6942,    -inf],\n         [ 0.5128,  1.6782, -0.0505,  0.8720,  1.5211,  0.8015, -0.8281,  0.3316]],\n        grad_fn=<SelectBackward0>),\n tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.6435, 0.3565, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.3405, 0.3043, 0.3552, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.1927, 0.1306, 0.4471, 0.2297, 0.0000, 0.0000, 0.0000, 0.0000],\n         [0.1318, 0.0934, 0.4571, 0.1929, 0.1248, 0.0000, 0.0000, 0.0000],\n         [0.1350, 0.1206, 0.2483, 0.1676, 0.1388, 0.1897, 0.0000, 0.0000],\n         [0.1644, 0.0704, 0.1675, 0.1061, 0.0723, 0.1028, 0.3165, 0.0000],\n         [0.0879, 0.2818, 0.0500, 0.1258, 0.2409, 0.1173, 0.0230, 0.0733]],\n        grad_fn=<SelectBackward0>))"},"metadata":{}}],"execution_count":42},{"cell_type":"markdown","source":"### Note on attention\n- attention is just a connection mechanism which makes independent entities to communicate between them that is to say make them transfer their information among themselves\n- the particular attention mechanism discussed here is **self-attented decoder** where only the past nodes talk with each other and all the k,q and v matrices come from same tensor\n- there are multiple types of attentions, the one most adjacent to this type of attention is encoder architecture (used in BERT) where instead of mask filling we allow all the tokens to talk to each other at one time","metadata":{}},{"cell_type":"markdown","source":"## 4.1 Addressing the variance propogation in attention\n- the weight matrix formed from k and query multiplication carries a variance in the order of $h^2$","metadata":{}},{"cell_type":"code","source":"k.var(),n_wei.var(),(n_wei*(head_size)**-0.5).var()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-02T12:58:53.813710Z","iopub.execute_input":"2026-01-02T12:58:53.814062Z","iopub.status.idle":"2026-01-02T12:58:53.823176Z","shell.execute_reply.started":"2026-01-02T12:58:53.814031Z","shell.execute_reply":"2026-01-02T12:58:53.822178Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"(tensor(0.3436, grad_fn=<VarBackward0>),\n tensor(2.9703, grad_fn=<VarBackward0>),\n tensor(0.1856, grad_fn=<VarBackward0>))"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}